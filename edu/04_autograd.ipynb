{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMdKzVdO1DR6kE19t56sg/L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Automatic Differentiation with ``torch.autograd``\n","=======================================\n","\n","ニューラルネットワークを訓練する際、その学習アルゴリズムとして、基本的には**バックプロパゲーション（back propagation）**が使われる．\n","\n","バックプロパゲーションでは、モデルの重みなどの各パラメータは、損失関数に対するその変数の微分値（勾配）に応じて調整されるので各パラメータの勾配が重要な役割を持つ．\n","\n","一見すると，各パラメータの微分値を求めるのは非常に煩雑で面倒な感じがするが，Pytorchでは``torch.autograd`` という微分エンジンが組み込まれていて計算グラフに対する勾配の自動計算を支援してくれるのですごい便利．\n","\n","\n"],"metadata":{"id":"zQvg3SkFaZY9"}},{"cell_type":"markdown","source":["簡単な計算グラフを用意するのでここで``torch.autograd`` を確認していく．"],"metadata":{"id":"2xxHmL4ibD8X"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UrM7r1_jaPkd"},"outputs":[],"source":["import torch\n","\n","x = torch.ones(5)  # input tensor\n","y = torch.zeros(3)  # expected output\n","w = torch.randn(5, 3, requires_grad=True)\n","b = torch.randn(3, requires_grad=True)\n","z = torch.matmul(x, w)+b\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"]},{"cell_type":"markdown","source":["テンソル、関数、計算グラフの関係\n","------------------------------------------\n","\n","上記のコードは以下の**計算グラフ(computational graph)**を示していル．"],"metadata":{"id":"3FqAMf-4biG-"}},{"cell_type":"markdown","source":["<img src=\"https://pytorch.org/tutorials/_images/comp-graph.png\" width=50% alt=\"各パラメータ\" title=\"損失関数の図\">"],"metadata":{"id":"N6C3NPHzbWD1"}},{"cell_type":"markdown","source":["この計算グラフでは``w``と``b``が最適化したいパラメータになる．\n","\n","コードからも確認できるように，requires_gradという引数をTrueにすることで勾配計算を許可する形になり，``backward``実行後に各パラメータの``.grad``に保存されることになる．\n"],"metadata":{"id":"v-5Tfr3ibrKa"}},{"cell_type":"code","source":["loss.backward()"],"metadata":{"id":"OHq4JWRwbSVE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"wの勾配\",w.grad)\n","\n","print(\"bの勾配\",b.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ByGYB2JdIgG","executionInfo":{"status":"ok","timestamp":1652160853704,"user_tz":-540,"elapsed":246,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"d9ee2e4e-ab33-40d1-9dd5-cbd3b4055498"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["wの勾配 tensor([[0.0532, 0.3261, 0.0416],\n","        [0.0532, 0.3261, 0.0416],\n","        [0.0532, 0.3261, 0.0416],\n","        [0.0532, 0.3261, 0.0416],\n","        [0.0532, 0.3261, 0.0416]])\n","bの勾配 tensor([0.0532, 0.3261, 0.0416])\n"]}]},{"cell_type":"markdown","source":["これは次の計算を求めていることに他ならない\n","\n","$\\frac{\\partial loss}{\\partial w}$ 、$\\frac{\\partial loss}{\\partial b}$ "],"metadata":{"id":"Kw6NemPseyum"}},{"cell_type":"markdown","source":["``requires_grad``はテンソルを定義する際、もしくはその後に、``x.requires_grad_(True)``を実行するなどして指定することもできる．\n","\n","注意：無闇に勾配追跡をONにしてしまうとコンピュータが裏で保持している計算グラフが莫大なものになってしまい計算の遅延に繋がりかねないので必要な時だけ``requires_grad``をTrueにする．"],"metadata":{"id":"0PYtOBQldk0N"}},{"cell_type":"markdown","source":["``backward``後にもう一度勾配計算を行うことは仕様上できない．\n","\n","どうしてももう一度行いたい場合は，``backward``の際に引数に``retain_graph=True``を渡すと良い．\n","\n","これを行うことで損失関数のヘッセ行列を得ることができる．"],"metadata":{"id":"pN2vwVg3fBbG"}},{"cell_type":"markdown","source":["勾配計算をしない方法\n","---------------------------\n","\n","デフォルトでは、``requires_grad=True``である全てのテンソルは計算履歴が保持され、勾配計算可能な状態になるが前述したように計算グラフは必要がないときには作らないほうが良い．\n","\n","''torch.no_grad''を使って計算グラフが不要なコードはまとめるのが普通．"],"metadata":{"id":"eyLydvuOfwQX"}},{"cell_type":"code","source":["z = torch.matmul(x, w)+b\n","print(z.requires_grad)\n","\n","with torch.no_grad():\n","    z = torch.matmul(x, w)+b\n","print(z.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"920rHyDzdJ5O","executionInfo":{"status":"ok","timestamp":1652162576647,"user_tz":-540,"elapsed":265,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"4c0486a4-cf27-4bc5-e16e-78ccfe30ba87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n"]}]},{"cell_type":"markdown","source":["また，``@torch.no_grad()``でラップすることで関数全体を包むこともできる．\n","\n","訓練済みのモデルを使って予測を行う際などには勾配計算を行う必要はないため全体をこれでラップすることも多い．"],"metadata":{"id":"WLDGDg1afwNG"}},{"cell_type":"markdown","source":["勾配の計算、追跡を不能にしたいケースの紹介\n","\n","- ネットワークの一部のパラメータを固定したい（frozen parameters）ケース。これは[ファインチューニング](https://colab.research.google.com/github/YutaroOgawa/pytorch_tutorials_jp/blob/main/notebook/2_Image_Video/2_1_transfer_learning_tutorial_jp.ipynb)時によくあるケース．\n","\n","\n","\n","- 順伝搬の計算スピードを高速化したいケース．\n"],"metadata":{"id":"Vjr3tyo6ld7z"}}]}