{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPIImcgHGNr+bnlLwvFv4MP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["マルチ学習のススメ\n","--------------------------\n","\n","単一GPUだけで計算をするのは限界がある．そこで2つ以上のGPUで学習を行う方法をここでは紹介する．\n","\n","**大きく分けてDataParallelによるマルチ学習とDistributedDataParallelによるマルチ学習がある**．\n","\n","前者は**手軽**に試せる．\n","\n","後者は手間はかかるが**訓練速度がさらに速くなる**のでおすすめ．\n","\n","今回はDistributedDataParallellについて詳しく見ていく"],"metadata":{"id":"QEKHG9QDUCsX"}},{"cell_type":"markdown","source":["DataParallelとDistributedDataParallelの違い\n","---------------\n","気になる人だけ見ればおk\n","\n","DataParallelはシングルプロセスかつマルチスレッドのみで機能するがDistributedDataParallelはさらにマルチプロセスでシングルマシン訓練、マルチマシン訓練でも機能する．\n","また，通常、DataParallel は、スレッド間のGILの競合、イテレーション毎に複製するモデル、そして入力の分割と出力の収集によって発生するオーバーヘッドが原因となり、単一のマシン上であってもDistributedDataParallel よりも遅くなる．\n","\n","モデルが大きすぎて単一のGPUに収まらない場合は、モデル並列を利用してモデルを複数のGPUに分割する必要があるが，DistributedDataParallel は、モデル並列と共に動作できる一方で DataParallel はモデル並列と共に使うことはできない．\n","\n","などが主な理由らしい"],"metadata":{"id":"JFKvZ25-Y9W9"}},{"cell_type":"code","source":["import os\n","import sys\n","import tempfile\n","import torch\n","import torch.distributed as dist\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.multiprocessing as mp\n","\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","\n","\n","def setup(rank, world_size):\n","    if sys.platform == 'win32':\n","        # Windowsプラットフォーム上では、Distribuedパッケージは\n","        # Glooバックエンドの集合通信のみサポートしています。\n","        # init_process_group内のinit_method パラメーターをローカルのファイルに設定してください。\n","        # 例 init_method=\"file:///f:/libtmp/some_file\"\n","        init_method=\"file:///{your local file path}\"\n","\n","        # プロセスグループの初期化\n","        dist.init_process_group(\n","            \"gloo\",\n","            init_method=init_method,\n","            rank=rank,\n","            world_size=world_size\n","        )\n","    else:\n","        os.environ['MASTER_ADDR'] = 'localhost'\n","        os.environ['MASTER_PORT'] = '12355'\n","\n","        # プロセスグループの初期化\n","        dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n","\n","def cleanup():\n","    dist.destroy_process_group()"],"metadata":{"id":"wfYFegXzYbbJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["簡単なモジュールを作ってDDPでラップする．"],"metadata":{"id":"b5eh3Qx9aME6"}},{"cell_type":"code","source":["class ToyModel(nn.Module):\n","    def __init__(self):\n","        super(ToyModel, self).__init__()\n","        self.net1 = nn.Linear(10, 10)\n","        self.relu = nn.ReLU()\n","        self.net2 = nn.Linear(10, 5)\n","\n","    def forward(self, x):\n","        return self.net2(self.relu(self.net1(x)))\n","\n","\n","def demo_basic(rank, world_size):\n","    print(f\"Running basic DDP example on rank {rank}.\")\n","    setup(rank, world_size)\n","\n","    # モデルを作成し、ランクidと共にGPUに移動\n","    model = ToyModel().to(rank)\n","    ddp_model = DDP(model, device_ids=[rank])\n","\n","    loss_fn = nn.MSELoss()\n","    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n","\n","    optimizer.zero_grad()\n","    outputs = ddp_model(torch.randn(20, 10))\n","    labels = torch.randn(20, 5).to(rank)\n","    loss_fn(outputs, labels).backward()\n","    optimizer.step()\n","\n","    cleanup()\n","\n","\n","def run_demo(demo_fn, world_size):\n","    mp.spawn(demo_fn,\n","             args=(world_size,),\n","             nprocs=world_size,\n","             join=True)"],"metadata":{"id":"gh6kGp6gaFEt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DDPを行う場合でも並列処理でない場合とほとんど同様の実装を使える．\n","\n","今回はかなり簡易的な実装だがもう少し発展的な実装の場合必要になる処理があるので下に記載．"],"metadata":{"id":"xdkow4ZQaYkh"}},{"cell_type":"markdown","source":["## 処理速度の違い\n","\n","DDPでは、コンストラクター、フォワードパス、そしてバックワードパスが分散処理の同期ポイントになる．\n","\n","そして、異なるプロセスは同じ数の同期処理を起動し、同じ順序でこれらの同期ポイントに到達し、ほぼ同時に各同期ポイントに入ることが期待されている．\n","\n","このようにしないと、速く行われるプロセスが早く到着し、出遅れたプロセスを待ってタイムアウトしてしまう可能性がある。\n","\n","したがって、ユーザーにはプロセス間でワークロードの分散を均等にする必要がある。"],"metadata":{"id":"ZbrYLyIba8hK"}},{"cell_type":"markdown","source":["しかし、処理速度の歪み、バラつきは、例えば、ネットワークの遅延、リソースの競合、または予測不能なワークロードの急増によって不可避的に発生する。\n","\n","このような状況でのタイムアウトを避けるには、init_process_group を呼び出す際に、十分な timeout 値を与えておく。"],"metadata":{"id":"TkN19FUUbODT"}},{"cell_type":"code","source":["def demo_model_parallel(rank, world_size):\n","    print(f\"Running DDP with model parallel example on rank {rank}.\")\n","    setup(rank, world_size)\n","\n","    # このプロセスで使用するmp_modelとデバイスをセットアップ\n","    dev0 = rank * 2\n","    dev1 = rank * 2 + 1\n","    mp_model = ToyMpModel(dev0, dev1)\n","    ddp_mp_model = DDP(mp_model)\n","\n","    loss_fn = nn.MSELoss()\n","    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n","\n","    optimizer.zero_grad()\n","    # 出力は dev1 に行われる。\n","    outputs = ddp_mp_model(torch.randn(20, 10))\n","    labels = torch.randn(20, 5).to(dev1)\n","    loss_fn(outputs, labels).backward()\n","    optimizer.step()\n","\n","    cleanup()\n","\n","\n","if __name__ == \"__main__\":\n","    n_gpus = torch.cuda.device_count()\n","    if n_gpus < 8:\n","        print(f\"Requires at least 8 GPUs to run, but got {n_gpus}.\")\n","    else:\n","        run_demo(demo_basic, 8)\n","        run_demo(demo_checkpoint, 8)\n","        run_demo(demo_model_parallel, 4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JLULuC17aTbO","executionInfo":{"status":"ok","timestamp":1666941208134,"user_tz":-540,"elapsed":6,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"366a7c15-990b-4294-dd6d-16ce237bbdb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requires at least 8 GPUs to run, but got 1.\n"]}]},{"cell_type":"markdown","source":["以前のGoogle Colaoratoryなら上手くいったコードだが，改悪によって利用できなくなってしまった．\n","\n","Googleドキュメントに別の形でDDPについて解説しておくのでそちらを見るのが吉．"],"metadata":{"id":"NmkWakEN7k6H"}}]}