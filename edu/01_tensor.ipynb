{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOY5iVMWfM8sHqGdT50l0Uk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["テンソル：Tensors \n","==========================\n","\n","Pytorchではテンソル型の変数を利用して入力や出力，モデルのパラメータを表現する．\n","\n","テンソルはNumPyのndarraysに似ているが、違いとしてGPUや他のハードウェアアクセラレータ上で動作させることが可能．\n","\n","また，テンソルは自動微分に最適化されている（後述する）\n","\n","NumPyのndarraysに触れたことのある人は飛ばしてこの章はok．\n","\n","触れたことのない，または自信がない人はおすすめ.\n","\n","また，他の章をこなした後にわからないところを見にくるのもいい．(自分も何度も見直してる)"],"metadata":{"id":"yJeAGK90HXVF"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"XX0q7JjmHMpe","executionInfo":{"status":"ok","timestamp":1666936076849,"user_tz":-540,"elapsed":2641,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}}},"outputs":[],"source":["import torch\n","import numpy as np"]},{"cell_type":"markdown","source":["テンソルは様々な方法で作成可能．\n","\n","1.データから直接テンソルに変換"],"metadata":{"id":"nE_j8uiLI3eY"}},{"cell_type":"code","source":["data = [[1,2],[3,4]]\n","x_data = torch.tensor(data)\n","print(x_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdPW2jzmIsxI","executionInfo":{"status":"ok","timestamp":1666936076850,"user_tz":-540,"elapsed":10,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"f4ee488e-ff39-454d-81c7-200b304965dd"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 2],\n","        [3, 4]])\n"]}]},{"cell_type":"markdown","source":["2.Numpy arraysからテンソルに変換"],"metadata":{"id":"ZmbBHv3rJFNW"}},{"cell_type":"code","source":["np_array = np.array(data)\n","x_np = torch.from_numpy(np_array)\n","print(x_np)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NbIfYvDWI1ND","executionInfo":{"status":"ok","timestamp":1666936076851,"user_tz":-540,"elapsed":8,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"93df22f4-8fc9-4771-e85d-d5c8dabc0569"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 2],\n","        [3, 4]])\n"]}]},{"cell_type":"markdown","source":["3.他のテンソルの形を真似て作成．"],"metadata":{"id":"EnGA8ZJAJiCs"}},{"cell_type":"code","source":["x_ones = torch.ones_like(x_data)\n","print(x_ones)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WJEctIs7JaDG","executionInfo":{"status":"ok","timestamp":1666936076851,"user_tz":-540,"elapsed":6,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"3ae6f1ec-e6d3-4bae-e34a-e58ee5db00ee"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 1],\n","        [1, 1]])\n"]}]},{"cell_type":"code","source":["x_random = torch.rand_like(x_data,dtype=torch.float)\n","print(x_random)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PtdyAiWXKSqz","executionInfo":{"status":"ok","timestamp":1666936077465,"user_tz":-540,"elapsed":3,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"1fd02ccd-aeb3-49bf-ccfc-1ade176363ca"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.5430, 0.6831],\n","        [0.6221, 0.8972]])\n"]}]},{"cell_type":"markdown","source":["4.shapeを指定してテンソル作成"],"metadata":{"id":"pb1DymdEKnhk"}},{"cell_type":"code","source":["shape = (2,3)\n","rand = torch.rand(shape)\n","ones = torch.ones(shape)\n","zeros = torch.zeros(shape)\n","print(rand,\"\\n\",ones,\"\\n\",zeros)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rG_p1GaGKaZ7","executionInfo":{"status":"ok","timestamp":1666936078020,"user_tz":-540,"elapsed":3,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"e395b1ed-b63f-4762-9c12-166ef32d3189"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.0290, 0.0914, 0.3842],\n","        [0.3972, 0.7503, 0.4090]]) \n"," tensor([[1., 1., 1.],\n","        [1., 1., 1.]]) \n"," tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n"]}]},{"cell_type":"markdown","source":["テンソルは属性変数として、その形状、データの型、保存されているデバイスを保持している．\n","\n","サイズを確認したいときに，.shapeを使うのはよく使う\n"],"metadata":{"id":"kD_drFOlLdya"}},{"cell_type":"code","source":["tensor = torch.rand(3,4)\n","\n","print(f\"Shape of tensor: {tensor.shape}\")\n","print(f\"Datatype of tensor: {tensor.dtype}\")\n","print(f\"Device tensor is stored on: {tensor.device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2wCqjsfvK51j","executionInfo":{"status":"ok","timestamp":1666936091740,"user_tz":-540,"elapsed":1027,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"292a5d3e-334b-45f1-a678-c315393a109c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of tensor: torch.Size([3, 4])\n","Datatype of tensor: torch.float32\n","Device tensor is stored on: cpu\n"]}]},{"cell_type":"markdown","source":["GPUの操作\n","==================\n","\n","PyTorchでは、算術、線形代数、行列操作（転置、インデックス、スライス）など、100種類以上のテンソル演算が可能．\n","\n","これらの各操作をGPU上で行うことが可能なのが深層学習ライブラリの特徴の一つ．\n","\n","デフォルト(上記のテンソルたち)ではテンソルはCPU上に作られる．\n","\n","主に以下の二つのメソッドでテンソルをGPU上に移動させる．\n"],"metadata":{"id":"yMlaViBcMVgg"}},{"cell_type":"markdown","source":[" Google ColabでGPUを使うためにはランタイムの接続方法を変更する必要がある．\n","\n"," やり方は簡単で「ランタイム」→「ランタイムのタイプを変更」→「None」→「GPU」とすれば良い．"],"metadata":{"id":"hdq1cEeKQOu5"}},{"cell_type":"markdown","source":["1..toメソッド"],"metadata":{"id":"k3P_nkWIORQ1"}},{"cell_type":"code","source":["ngpu = 1\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n","a = torch.ones(shape).to(device)\n","print(a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aU5aOmAdNpnI","executionInfo":{"status":"ok","timestamp":1666936099751,"user_tz":-540,"elapsed":4223,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"51b115cb-0bfa-45b7-f297-2f754911a038"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1.],\n","        [1., 1., 1.]], device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["2.cudaメソッド"],"metadata":{"id":"YlSl6fD8OSce"}},{"cell_type":"code","source":["use_cuda = torch.cuda.is_available()\n","if use_cuda:\n","    torch.cuda.empty_cache()\n","    torch.cuda.set_device(0)\n","b = torch.ones(shape).cuda()\n","print(b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"etQwlNbmOCDK","executionInfo":{"status":"ok","timestamp":1666936118569,"user_tz":-540,"elapsed":513,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"746745bb-7fcd-4d23-cc16-e520c49b9dee"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1.],\n","        [1., 1., 1.]], device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["どちらのメソッドでも構わないが過去に自分は1を使っていて何らかの不都合が生じて(詳細は覚えてない)以来2を使っているのでこだわりがなければ2をおすすめする．"],"metadata":{"id":"OIjy56CwOyoH"}},{"cell_type":"markdown","source":["テンソルの算術計算\n","==================\n","Tensor APIの算術計算の記法はNumPy APIを踏襲している．\n"],"metadata":{"id":"yowCTTIMPS4z"}},{"cell_type":"code","source":["data = [1.,2,3,4]"],"metadata":{"id":"2siqoJsjQy9Q","executionInfo":{"status":"ok","timestamp":1666936176137,"user_tz":-540,"elapsed":810,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["tensor = torch.tensor(data)\n","print(tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DupQmS9YOk8a","executionInfo":{"status":"ok","timestamp":1666936176812,"user_tz":-540,"elapsed":3,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"f7b4c670-9a7a-46f6-dafb-3aa99107570c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 2., 3., 4.])\n"]}]},{"cell_type":"markdown","source":["1.要素ごとの積を求めるには次の三つのどれかを使う\n","\n","  おそらく各手法で微妙に計算時間が変わると思うが微小なので気にしなくて良い(変わったらごめん)\n","\n","  おすすめはtensor*tensor"],"metadata":{"id":"sNVN-KxbRXfK"}},{"cell_type":"code","source":["a1 = tensor * tensor\n","a2 = tensor.mul(tensor)\n","\n","_ = torch.rand_like(tensor)\n","a3 = torch.mul(tensor, tensor, out=_)\n","\n","print(a1 - a2)\n","\n","print(a1 - a3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPT2xl8VPwv4","executionInfo":{"status":"ok","timestamp":1666936178576,"user_tz":-540,"elapsed":3,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"e7b63b59-ccff-462d-e141-4981359450a2"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0., 0., 0., 0.])\n","tensor([0., 0., 0., 0.])\n"]}]},{"cell_type":"markdown","source":["2.行列の掛け算を求める場合は次の三つを使う．\n","\n","上に同じ\n","\n","おすすめはtensor @ tensor.T"],"metadata":{"id":"eQ6PRCj6SXWP"}},{"cell_type":"code","source":["b1 = tensor @ tensor.T\n","b2 = tensor.matmul(tensor.T)\n","\n","__ = torch.rand_like(tensor)\n","b3 = torch.matmul(tensor, tensor.T, out=__)\n","\n","print(b1 - b2)\n","print(b1 - b3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JXKHNodWP6Xx","executionInfo":{"status":"ok","timestamp":1666936185788,"user_tz":-540,"elapsed":461,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"8586d54b-aec4-410e-acc2-9b868da6ce6d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.)\n","tensor(0.)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2981.)\n","  \"\"\"Entry point for launching an IPython kernel.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape []. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n","  \"\"\"\n"]}]},{"cell_type":"markdown","source":["**1要素のテンソル** \n","\n","1要素のテンソル（テンソルの全要素を足し算する等をした結果生まれる）を扱う場合には、``.item()``を使用する.\n","\n","テンソル型ではなくpythonの数値型として取り出せる"],"metadata":{"id":"JYDjKxgnTYM_"}},{"cell_type":"code","source":["agg = tensor.sum()\n","agg_item = agg.item()\n","print(agg)\n","print(agg_item, type(agg_item))"],"metadata":{"id":"g3I0pjvfP92Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"N5UpE8ztJVTA"},"execution_count":null,"outputs":[]}]}