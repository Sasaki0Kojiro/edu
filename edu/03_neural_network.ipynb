{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM9HBpuBg/LTRFmwAqNePn1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["ニューラルネットワークモデルの作り方\n","===================\n","\n","深層学習では学習における仮説関数にニューラルネットワークを使うことが一般的．\n","\n","pytorchでももちろん整備されており，[`torch.nn`](https://pytorch.org/docs/stable/nn.html)で用意されているクラス、関数を使って独自のニューラルネットワークを作ることが可能．\n","\n"],"metadata":{"id":"xDKXTYBgkvpQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_mqu6T3j0mB"},"outputs":[],"source":["import os\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms"]},{"cell_type":"markdown","source":["クラスの定義\n","-------------------------\n","``nn.Module``を継承し、独自のネットワークモデルを定義し、その後ネットワークのレイヤーを ``__init__``で初期化する．\n","\n","``nn.Module`` を継承した全モジュールは、入力データの順伝搬関数である``forward``関数を持つ。\n","\n","backward関数を定義する必要はない．nn.moduleで定義してくれてる．"],"metadata":{"id":"Le7vcXGdluKZ"}},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits"],"metadata":{"id":"UmKSFCb1lpFt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["use_cuda = torch.cuda.is_available()\n","if use_cuda:\n","    torch.cuda.empty_cache()\n","    torch.cuda.set_device(0)"],"metadata":{"id":"x7E9i2ySltNh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["定義したクラスのインスタンスを作って，GPU上に移す．"],"metadata":{"id":"Hd_2vm7qmTNv"}},{"cell_type":"code","source":["model = Net().cuda()\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5MprmyEumO6M","executionInfo":{"status":"ok","timestamp":1650955402203,"user_tz":-540,"elapsed":9911,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"ecd30016-e470-48d6-d124-345a212b5ea9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Net(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","    (5): ReLU()\n","  )\n",")\n"]}]},{"cell_type":"markdown","source":["ここで作成したインスタンスを使って実際に学習を進めていく．"],"metadata":{"id":"nfD40KiPnFlQ"}},{"cell_type":"markdown","source":["モデルレイヤー\n","-------------------------\n","\n","作ったモデルを各レイヤーレベルで確認する．\n"],"metadata":{"id":"lrOrcO6-nRIS"}},{"cell_type":"markdown","source":["適当な入力を用意する．\n","\n","MNISTに合わせて，サイズ28x28の3枚の画像からなるミニバッチのサンプルとする．"],"metadata":{"id":"rCxv_BDwndk1"}},{"cell_type":"code","source":["input = torch.rand(3,28,28)\n","print(input.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9y9yu9KBnq18","executionInfo":{"status":"ok","timestamp":1650955615175,"user_tz":-540,"elapsed":4,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"5b02447e-3781-4dfe-8505-1b56d96b47d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 28, 28])\n"]}]},{"cell_type":"markdown","source":["**nn.Flatten**\n","\n","[`nn.Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)レイヤーで、2次元（28x28）の画像を、1次元の784ピクセルの値へと変換する．\n","\n","ミニバッチの0次元目は、サンプル番号を示す次元で、この次元は`nn.Flatten`を通しても変化しない（1次元目以降がFlattenされる）．\n","\n","今回の例であれば3は変化しない．(ミニバッチ数)"],"metadata":{"id":"MfFJWyfAn6ZV"}},{"cell_type":"code","source":["flatten = nn.Flatten()\n","flat_image = flatten(input)\n","print(flat_image.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sg2Tx5VYmgsk","executionInfo":{"status":"ok","timestamp":1650955708684,"user_tz":-540,"elapsed":4,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"82c0a254-8af9-482b-e8c5-318a78096f0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 784])\n"]}]},{"cell_type":"markdown","source":["**nn.Linear** \n","\n","\n","[`linear layer`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)は、線形変換を施す．\n","\n","`linear layer`は重みとバイアスのパラメータを保持している．\n","\n","\n","\n"],"metadata":{"id":"IFO0lYFhoZ4p"}},{"cell_type":"code","source":["layer1 = nn.Linear(in_features=28*28, out_features=20)\n","hidden1 = layer1(flat_image)\n","print(hidden1.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rtm8o68hnwYl","executionInfo":{"status":"ok","timestamp":1650956047047,"user_tz":-540,"elapsed":4,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"85be5af3-3e8a-4710-9864-766a26140ef6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 20])\n"]}]},{"cell_type":"markdown","source":["**nn.ReLU**\n","\n","非線形な活性化関数\n","\n","線形変換だけでなく非線形な変換を施すことがニューラルネットワークを運営していく上で非常に重要になっている．\n","\n","通常これらの関数は線形変換の後に，ニューラルネットワークの表現力を向上させる役割を持つ．\n","\n","今回のモデルではReLUを使っている．（一般的にもReLUが多いと思う）\n","\n","非線形な活性化関数には他にも種類がある．"],"metadata":{"id":"Stv3-NVMpkrt"}},{"cell_type":"code","source":["print(f\"Before ReLU: {hidden1}\\n\\n\")\n","hidden1 = nn.ReLU()(hidden1)\n","print(f\"After ReLU: {hidden1}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zx0l7m6xqga_","executionInfo":{"status":"ok","timestamp":1650956308920,"user_tz":-540,"elapsed":234,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"d56d6985-8835-4214-aaf1-4cfa810c409f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Before ReLU: tensor([[-0.0138,  0.1837,  0.2267,  0.0148, -0.0038, -0.2665,  0.2260,  0.2054,\n","          0.1403, -0.2230, -0.1733,  0.2614,  0.3257,  0.3238, -0.4651, -0.2401,\n","          0.0335,  0.1344, -0.0810,  0.1590],\n","        [ 0.0407,  0.5334,  0.5330, -0.0935, -0.2463, -0.2824, -0.5414,  0.2546,\n","          0.1329, -0.2451,  0.2114, -0.1061,  0.1594,  0.4361, -0.4799, -0.3238,\n","         -0.2698, -0.1091,  0.1746,  0.5918],\n","        [ 0.1618,  0.1593,  0.2756, -0.1153, -0.0890, -0.3470, -0.1498, -0.0178,\n","          0.0266, -0.3050,  0.3482, -0.0271,  0.2928,  0.2468, -0.2248, -0.3397,\n","         -0.1738,  0.2635,  0.0956,  0.4242]], grad_fn=<AddmmBackward0>)\n","\n","\n","After ReLU: tensor([[0.0000, 0.1837, 0.2267, 0.0148, 0.0000, 0.0000, 0.2260, 0.2054, 0.1403,\n","         0.0000, 0.0000, 0.2614, 0.3257, 0.3238, 0.0000, 0.0000, 0.0335, 0.1344,\n","         0.0000, 0.1590],\n","        [0.0407, 0.5334, 0.5330, 0.0000, 0.0000, 0.0000, 0.0000, 0.2546, 0.1329,\n","         0.0000, 0.2114, 0.0000, 0.1594, 0.4361, 0.0000, 0.0000, 0.0000, 0.0000,\n","         0.1746, 0.5918],\n","        [0.1618, 0.1593, 0.2756, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0266,\n","         0.0000, 0.3482, 0.0000, 0.2928, 0.2468, 0.0000, 0.0000, 0.0000, 0.2635,\n","         0.0956, 0.4242]], grad_fn=<ReluBackward0>)\n"]}]},{"cell_type":"markdown","source":["（ReLU関数はmax{0,x_i}をしてる）"],"metadata":{"id":"ofiIcDwsqqT1"}},{"cell_type":"markdown","source":["**nn.Sequential**\n","\n","[``nn.Sequential``](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)は、モジュールを順番に格納する箱のような要素になる．\n","\n","入力データは``nn.Sequential``に定義された順番に各モジュールを伝搬する．\n","\n","<br>\n","\n","例えば以下の実装例のように、``seq_modules``と名付けた、複数のモジュールを束ねたモジュールを簡単に構築でるき．"],"metadata":{"id":"pfqP0P3Cqh84"}},{"cell_type":"code","source":["seq_modules = nn.Sequential(\n","    flatten,\n","    layer1,\n","    nn.ReLU(),\n","    nn.Linear(20, 10)\n",")\n","input_image = torch.rand(3,28,28)\n","logits = seq_modules(input_image)"],"metadata":{"id":"OcE_GqNUpZg_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**nn.Softmax**\n","\n","ニューラルネットワークの最後のlinear layerは`logits` [- ∞, ∞] を出力するがnn.Softmaxモジュールを通すことで採取的な値は[0, 1]の範囲となり、各クラスの確率を返すことになる．\n","\n","\n","``dim``パラメータは次元を示しており、`dim=1`の次元で和を求めると確率の総和なので1になる．"],"metadata":{"id":"Po8xH5u6q6A5"}},{"cell_type":"code","source":["softmax = nn.Softmax(dim=1)\n","pred_probab = softmax(logits)"],"metadata":{"id":"QGIyEmSKq46S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["モデルパラメータ\n","-------------------------\n","\n","ニューラルネットワークを構成する多くのモジュールは、おのおのパラメータを保持している．\n","\n","例えば、重みやバイアスがそれに該当する．\n","\n","これらの値が訓練時に最適化される．"],"metadata":{"id":"tm_n70QOrRXr"}},{"cell_type":"markdown","source":["\n","``nn.Module`` を継承することで、モデルオブジェクト内で定義されたすべてのフィールドが自動的に追跡でき、``parameters()`` や ``named_parameters()`` メソッドを使って、モデルの各レイヤーのすべてのパラメータにアクセスできるようになる\n","\n","これを使ってパラメータの更新や調整を行うことができる．\n","\n","よく使うのはparameters()"],"metadata":{"id":"Kf55yIe6sNf0"}},{"cell_type":"code","source":["print(\"Model structure: \", model, \"\\n\\n\")\n","\n","for name, param in model.named_parameters():\n","    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sXEdtkQdrPPT","executionInfo":{"status":"ok","timestamp":1650956728327,"user_tz":-540,"elapsed":216,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"43ed367a-cd41-4a7a-c043-ea4aeb469e34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model structure:  Net(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","    (5): ReLU()\n","  )\n",") \n","\n","\n","Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0347,  0.0157, -0.0346,  ...,  0.0337, -0.0033, -0.0101],\n","        [ 0.0157, -0.0141,  0.0209,  ...,  0.0315,  0.0317, -0.0228]],\n","       device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0303, -0.0311], device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0045, -0.0284,  0.0280,  ..., -0.0043, -0.0073,  0.0348],\n","        [ 0.0430, -0.0284, -0.0266,  ...,  0.0050, -0.0287, -0.0155]],\n","       device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0402, -0.0390], device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0024, -0.0076,  0.0051,  ...,  0.0056,  0.0154, -0.0254],\n","        [ 0.0241,  0.0270,  0.0087,  ...,  0.0201,  0.0286,  0.0218]],\n","       device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0175, -0.0235], device='cuda:0', grad_fn=<SliceBackward0>) \n","\n"]}]},{"cell_type":"code","source":["for p in model.parameters():\n","  print(p.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fdBjhqqYsHSF","executionInfo":{"status":"ok","timestamp":1650957012752,"user_tz":-540,"elapsed":4,"user":{"displayName":"佐々木小次郎","userId":"09458700915206684906"}},"outputId":"90ddfecf-22fc-4a1c-9668-bfa432f5e288"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([512, 784])\n","torch.Size([512])\n","torch.Size([512, 512])\n","torch.Size([512])\n","torch.Size([10, 512])\n","torch.Size([10])\n"]}]}]}