{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMDAbx+n2VHz7ohBx02AEVq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["DDPはDPとは違い，ラッピングするだけとはいかない．\n","\n","重要な箇所のみ解説する\n","\n","また，このコードをそのままcolab上で動かすことはおそらくできないのでaeonなどのマシンを使って実装するように．"],"metadata":{"id":"v3Ual5VLMqWP"}},{"cell_type":"code","source":["def main():\n","    ngpus_per_node = torch.cuda.device_count()\n","    mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))"],"metadata":{"id":"Oyw5y-iUM8ly"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["mp.spawn(A,B,C)を使ってプロセスを複数個発生させる\n","\n","これは，実際には関数A(X,B,C)を複数行っている\n","\n","ここでのXは各GPU情報になる(たとえばGPU:0のような形)"],"metadata":{"id":"w7OYK3dDNb_v"}},{"cell_type":"code","source":["def main_worker(gpu, ngpus_per_node, args):\n","    args.gpu = gpu\n","    dist.init_process_group(\n","    backend=\"nccl\",\n","    init_method=\"tcp://172.17.0.3:8888\",\n","    world_size=ngpus_per_node,\n","    rank=args.gpu\n","    )\n","    torch.cuda.set_device(args.gpu)\n","    model = eval(args.model)(100)\n","    model.cuda(args.gpu)\n","    # When using a single GPU per process and per\n","    # DistributedDataParallel, we need to divide the batch size\n","    # ourselves based on the total number of GPUs of the current node.\n","    args.batch_size = int(args.batch_size / ngpus_per_node)\n","    args.workers = int((args.num_worker + ngpus_per_node - 1) / ngpus_per_node)\n","    model = torch.nn.parallel.DistributedDataParallel(\n","        model, device_ids=[args.gpu]\n","    )\n","    averaged_model = AveragedModel(model)\n","\n","    loss_func = nn.CrossEntropyLoss().cuda(args.gpu)\n","\n","    optimizer = torch.optim.SGD(\n","        model.parameters(),\n","        args.lr,\n","        momentum=args.momentum,\n","        weight_decay=args.weight_decay,\n","    )\n","    ................................................(省略)"],"metadata":{"id":"rQRYpFYUORCB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["大事になるのがdist.init_process_groupの部分でここで各GPUの通信が初期化される．\n","\n","backendはGPUを利用する場合はncclで固定\n","\n","init_methodはマシンのIPアドレスを入力(localhostとして127.0.0.1でもおk)\n","\n","また，modelをtorch.nn.parallel.DistributedDataParalleでラッピングする必要がある．\n","\n","これ以降は通常の学習と同様の手順で進めていく\n","\n","\n","最後にDDP用のコードを共有して終わる．\n","\n","このコードは先ほどと同様にcolab上では動かないので注意すること．\n"],"metadata":{"id":"UwZx2WnmOjXh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7kHzBxf1Mc2t"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","import argparse\n","import sys\n","import time\n","\n","import torch\n","import torch.backends.cudnn as cudnn\n","import torch.distributed as dist\n","import torch.multiprocessing as mp\n","import torch.optim as optim\n","import torch.nn as nn\n","\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from torch.optim.lr_scheduler import MultiStepLR\n","from torch.optim.swa_utils import AveragedModel\n","\n","from model.wide_res_net import WRN28_10\n","from model.pyramid import Pyramid\n","from model.resnet import ResNet18,ResNet50,ResNet101,ResNet152\n","from model.smooth_cross_entropy import smooth_crossentropy\n","\n","from data.cifar import CIFAR10,CIFAR100\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","from utility.cutout import Cutout\n","\n","parser = argparse.ArgumentParser(description='PyTorch Training')\n","parser.add_argument('--model',default='ResNet18',type=str)\n","parser.add_argument('--dataset',default='CIFAR100',type=str)\n","#parser.add_argument('--optimizer',default='SGD',type=str,help='SGD')\n","parser.add_argument('--lr_type', default='MultiStepLR', type=str)#'MultiStepLR'/'CosineAnnealingLR'\n","parser.add_argument('--epoch', default=10, type=int)\n","parser.add_argument('--start_averaged', default=2, type=int)\n","parser.add_argument('--batch_size', default=256, type=int)\n","parser.add_argument('--weight_decay', default=5e-3, type=float)\n","parser.add_argument('--momentum', default=0.0, type=float)\n","parser.add_argument('--lr', default=0.1, type=float)\n","parser.add_argument('--eta_min',default=0.00,type=float)\n","parser.add_argument('--num_worker', default=8, type=int, help='number of workers')\n","parser.add_argument('--label_smoothing',default=0.0,type=float)\n","parser.add_argument('--loss_func', default='CE', type=str)\n","parser.add_argument('--debug_flag',default='OFF',type=str)\n","parser.add_argument('--start_cm', default=300, type=int)\n","#'SCE/CE'\n","\n","\n","\n","args = parser.parse_args()\n","\n","total_train=50000\n","total_test=10000\n","\n","def main():\n","    ngpus_per_node = torch.cuda.device_count()\n","    mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n","\n","def main_worker(gpu, ngpus_per_node, args):\n","    args.gpu = gpu\n","    dist.init_process_group(\n","    backend=\"nccl\",\n","    init_method=\"tcp://172.17.0.3:8888\",\n","    world_size=ngpus_per_node,\n","    rank=args.gpu\n","    )\n","    torch.cuda.set_device(args.gpu)\n","    model = eval(args.model)(100)\n","    model.cuda(args.gpu)\n","    # When using a single GPU per process and per\n","    # DistributedDataParallel, we need to divide the batch size\n","    # ourselves based on the total number of GPUs of the current node.\n","    args.batch_size = int(args.batch_size / ngpus_per_node)\n","    args.workers = int((args.num_worker + ngpus_per_node - 1) / ngpus_per_node)\n","    model = torch.nn.parallel.DistributedDataParallel(\n","        model, device_ids=[args.gpu]\n","    )\n","    averaged_model = AveragedModel(model)\n","\n","    loss_func = nn.CrossEntropyLoss().cuda(args.gpu)\n","\n","    optimizer = torch.optim.SGD(\n","        model.parameters(),\n","        args.lr,\n","        momentum=args.momentum,\n","        weight_decay=args.weight_decay,\n","    )\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epoch)\n","\n","    train_set = torchvision.datasets.CIFAR100(root='./cifar', train=True, download=True, transform=transforms.ToTensor())\n","    data = torch.cat([d[0] for d in DataLoader(train_set)])\n","    mean,std = data.mean(dim=[0, 2, 3]), data.std(dim=[0, 2, 3])\n","\n","    train_transform = transforms.Compose([\n","        torchvision.transforms.RandomCrop(size=(32, 32), padding=4),\n","        torchvision.transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std),\n","        Cutout()\n","    ])\n","\n","    test_transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std)\n","    ])\n","\n","    train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n","    test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n","    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n","    test_sampler = torch.utils.data.distributed.DistributedSampler(\n","        test_dataset, shuffle=False, drop_last=True\n","    )\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None), num_workers=args.workers,pin_memory=True,sampler=train_sampler)\n","    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers,pin_memory=True,sampler=test_sampler)\n","    total_time = 0\n","    for epoch in range(args.epoch):\n","        torch.cuda.synchronize()\n","        time_ep = time.time()\n","        train_loss, train_acc, time_ep = train(epoch,model,averaged_model,loss_func,train_loader, optimizer,args)\n","        scheduler.step()\n","        print(\"epoch\", epoch+1, \"lr:{:.7f}\".format(optimizer.param_groups[0]['lr']), \" train_loss:{:.5f}\".format(train_loss), \"train_acc:{:.2f}\".format(train_acc), \"time:{:.3f}\".format(time_ep))\n","        total_time += time_ep\n","\n","        if (epoch + 1)%5 == 0:\n","            test_loss, test_acc, avg_test_loss, avg_test_acc= test(epoch,model,averaged_model,loss_func,test_loader,args)\n","            print('test_loss:{:.5f}'.format(test_loss), 'test_acc:{:.2f}'.format(test_acc), 'avg_test_loss:{:.5f}'.format(avg_test_loss), 'avg_test_acc:{:.2f}'.format(avg_test_acc))\n","    print('{:.0f}:{:.0f}:{:.0f}'.format(total_time//3600, total_time%3600//60, total_time%3600%60))\n","    if args.gpu == 0:\n","        torch.save(model.state_dict(),'/home/noise_sam/DDP/model.pth')\n","        torch.save(averaged_model.state_dict(),'/home/noise_sam/DDP/averaged_model.pth')\n","\n","def train(epoch,model,averaged_model,loss_func,train_loader, optimizer,args):\n","\n","    torch.cuda.synchronize()\n","    time_ep = time.time()\n","    model.train()\n","    averaged_model.train()\n","\n","    train_loss = 0.0\n","    correct = 0.0\n","\n","    for inputs, target in train_loader:\n","        optimizer.zero_grad()\n","        input_size = inputs.size()[0]\n","        inputs, target = inputs.cuda(), target.cuda()\n","        output = model(inputs)\n","        loss = loss_func(output, target)\n","\n","        if args.loss_func == \"SCE\":\n","            loss.mean().backward()\n","            optimizer.step()\n","            if epoch >= args.start_averaged:\n","                averaged_model.update_parameters(model)\n","            with torch.no_grad():\n","                train_loss += loss.sum().item()/total_train\n","                correct += (torch.argmax(output, 1) == target).sum().item()/total_train\n","        else:\n","            loss.backward()\n","            optimizer.step()\n","            if epoch >= args.start_averaged:\n","                averaged_model.update_parameters(model)\n","            with torch.no_grad():\n","                train_loss += loss.data * input_size/total_train\n","                _, pred = torch.max(output, 1)\n","                correct += pred.eq(target).sum()/total_train\n","    torch.cuda.synchronize()\n","    time_ep = time.time() - time_ep\n","\n","    if epoch >= args.start_averaged:\n","        torch.optim.swa_utils.update_bn(train_loader, averaged_model, optimizer.param_groups[0]['params'][0].device)\n","\n","\n","    return train_loss, 100*correct, time_ep\n","\n","def test(epoch,model,averaged_model,loss_func,test_loader,args):\n","    model.eval()\n","    averaged_model.eval()\n","\n","    test_loss = 0.0\n","    test_correct = 0.0\n","    avg_test_loss = 0.0\n","    avg_test_correct = 0.0\n","\n","    with torch.no_grad():\n","        for inputs, target in test_loader:\n","            input_size = inputs.size()[0]\n","            inputs, target = inputs.cuda(), target.cuda()\n","\n","            if args.loss_func == \"SCE\":\n","                #normal_prediction\n","                predict = model(inputs)\n","                loss = loss_func(predict, target)\n","                test_loss += loss_func(predict, target).sum().item()/ total_test\n","                test_correct += (torch.argmax(predict, 1) == target).sum().item() / total_test\n","                #averaged_prediction\n","                avg_predict = averaged_model(inputs)\n","                loss = loss_func(avg_predict, target)\n","                avg_test_loss += loss_func(avg_predict, target).sum().item()/ total_test\n","                avg_test_correct += (torch.argmax(avg_predict, 1) == target).sum().item() / total_test\n","            else:\n","                #normal_prediction\n","                predict = model(inputs)\n","                loss = loss_func(predict, target)\n","                test_loss += loss.data * input_size / total_test\n","                _, pred = torch.max(predict.data, 1)\n","                test_correct += pred.eq(target).sum() / total_test\n","                #averaged_prediction\n","                avg_predict = averaged_model(inputs)\n","                loss = loss_func(avg_predict, target)\n","                avg_test_loss += loss.data * input_size / total_test\n","                _, pred = torch.max(avg_predict.data, 1)\n","                avg_test_correct += pred.eq(target).sum() / total_test\n","\n","    return test_loss, 100*test_correct, avg_test_loss, 100*avg_test_correct\n","\n","def copy_model(epoch):\n","    for p,avg_p in zip(model.parameters(),averaged_model.parameters()):\n","        avg_p_ = avg_p.detach()\n","        p.detach().copy_(avg_p_)\n","    torch.optim.swa_utils.update_bn(dataset.train, model, optimizer.param_groups[0]['params'][0].device)\n","    print('======================================')\n","    test_loss, test_acc, avg_test_loss, avg_test_acc= test(epoch)\n","    print('test_loss:{:.5f}'.format(test_loss), 'test_acc:{:.2f}'.format(test_acc), 'avg_test_loss:{:.5f}'.format(avg_test_loss), 'avg_test_acc:{:.2f}'.format(avg_test_acc))\n","\n","def norm_avg():\n","    for named_p,avg_p in zip(model.named_parameters(),averaged_model.parameters()):\n","        name,p = named_p\n","        if not 'norm' in name:\n","            p_ = p.detach()\n","            avg_p.detach().copy_(p_)\n","    torch.optim.swa_utils.update_bn(dataset.train, averaged_model, optimizer.param_groups[0]['params'][0].device)\n","    print('======================================')\n","    test_loss, test_acc, avg_test_loss, avg_test_acc= test(epoch)\n","    print('test_loss:{:.5f}'.format(test_loss), 'test_acc:{:.2f}'.format(test_acc), 'avg_test_loss:{:.5f}'.format(avg_test_loss), 'avg_test_acc:{:.2f}'.format(avg_test_acc))\n","\n","if __name__ == \"__main__\":\n","    main()"]}]}